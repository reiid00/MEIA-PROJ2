{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f9ad72b2",
   "metadata": {},
   "source": [
    "\n",
    "# Pré-Tratamento: Tratamento de Dados e Preparação do Dataset\n",
    "\n",
    "## Objetivo\n",
    "  A apartir de um conjunto de dados não tratados referente às receitas de uma loja de desporto, efetuar as associações necessárias, eliminando toda a informação considerada desnecessária para o âmbito do nosso projeto, gerando um dataset final a ser, posteriormente, importado no pré-processamento de dados. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09fac7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "     -------------------------------------- 235.9/235.9 kB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install xlrd\n",
    "%pip install openpyxl\n",
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471bbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feaa997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregamos as informações de todos os produtos, separados em 7 ficheiros, juntando os mesmos num só data frame.\n",
    "dataset_prod_1 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_1.xls\",\"Produtos\")\n",
    "dataset_prod_2 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_2.xls\",\"Query\")\n",
    "dataset_prod_3 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_3.xls\",\"Query\")\n",
    "dataset_prod_4 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_4.xls\",\"Query\")\n",
    "dataset_prod_5 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_5.xls\",\"Query\")\n",
    "dataset_prod_6 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_6.xls\",\"Query\")\n",
    "dataset_prod_7 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_7.xls\",\"Query\")\n",
    "dataset_prod_merged = pd.concat([dataset_prod_1,dataset_prod_2,dataset_prod_3,dataset_prod_4,dataset_prod_5,dataset_prod_6,dataset_prod_7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59334e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Código                           Nome                      Descrição  \\\n",
      "0       1              TESTE HOMOLOGACAO              TESTE HOMOLOGACAO   \n",
      "1       2      PENALTY 353026-645 TACTEL      PENALTY 353026-645 TACTEL   \n",
      "2       3      PENALTY 353028-650 TACTEL      PENALTY 353028-650 TACTEL   \n",
      "3       4      PENALTY 353028-625 TACTEL      PENALTY 353028-625 TACTEL   \n",
      "4       5  PENALTY 353025-652 MICROFIBRA  PENALTY 353025-652 MICROFIBRA   \n",
      "\n",
      "  Unidade  Fornecedor  Marca  Estoque  Situação Tributária  IPI de Entrada  \\\n",
      "0      UN         NaN    0.0      NaN                  0.0             0.0   \n",
      "1      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "2      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "3      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "4      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "\n",
      "   IPI de Saída  ...  Classificação13  Classificação14  Classificação15  \\\n",
      "0           0.0  ...                3                0              230   \n",
      "1           0.0  ...                3                2                1   \n",
      "2           0.0  ...                3                2                1   \n",
      "3           0.0  ...                3                2                1   \n",
      "4           0.0  ...                3                2                1   \n",
      "\n",
      "   Marcação       CEST  EAN  NCM  Código do Benefício  \\\n",
      "0         0  1234567.0  NaN  NaN                  NaN   \n",
      "1         0        NaN  NaN  NaN                  NaN   \n",
      "2         0        NaN  NaN  NaN                  NaN   \n",
      "3         0        NaN  NaN  NaN                  NaN   \n",
      "4         0        NaN  NaN  NaN                  NaN   \n",
      "\n",
      "   Descrição para Internet  Habilitado para Internet  \n",
      "0                      NaN                       NaN  \n",
      "1                      NaN                       NaN  \n",
      "2                      NaN                       NaN  \n",
      "3                      NaN                       NaN  \n",
      "4                      NaN                       NaN  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificamos o conjunto total de produtos.\n",
    "print(dataset_prod_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adce8c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Código', 'Nome', 'Unidade', 'Referência', 'Data da última atualização',\n",
      "       'Nome da marca', 'Complemento', 'Classificação11', 'Classificação12',\n",
      "       'Classificação13', 'Classificação14', 'Classificação15'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificamos as colunas dos produtos, mantendo as colunas necessárias para o projeto, \n",
    "# sendo estas todas as que representam características realmente utilizadas pela empresa e que poderão ser utilizadas tanto para o cálculo de receitas como para a associação de produtos\n",
    "dataset_prod_merged = dataset_prod_merged[[\"Código\",\"Nome\",\"Unidade\",\"Referência\",\"Data da última atualização\",\"Nome da marca\", \"Complemento\", \"Classificação11\",\"Classificação12\",\"Classificação13\",\"Classificação14\",\"Classificação15\"]]\n",
    "\n",
    "# Verificamos que as colunas foram devidamente removidas\n",
    "print(dataset_prod_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f28133e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Código                           Nome Unidade Referência  \\\n",
      "0       1              TESTE HOMOLOGACAO      UN        NaN   \n",
      "1       2      PENALTY 353026-645 TACTEL      UN        NaN   \n",
      "2       3      PENALTY 353028-650 TACTEL      UN        NaN   \n",
      "3       4      PENALTY 353028-625 TACTEL      UN        NaN   \n",
      "4       5  PENALTY 353025-652 MICROFIBRA      UN        NaN   \n",
      "\n",
      "  Data da última atualização Nome da marca Complemento  Classificação11  \\\n",
      "0        2016-06-27 00:00:00         TESTE         NaN                0   \n",
      "1        2012-06-15 00:00:00       PENALTY         NaN                3   \n",
      "2                        NaT       PENALTY         NaN                3   \n",
      "3                        NaT       PENALTY         NaN                3   \n",
      "4                        NaT       PENALTY         NaN                3   \n",
      "\n",
      "   Classificação12  Classificação13  Classificação14  Classificação15  \\\n",
      "0                3                3                0              230   \n",
      "1                2                3                2                1   \n",
      "2                3                3                2                1   \n",
      "3                2                3                2                1   \n",
      "4                3                3                2                1   \n",
      "\n",
      "  Nome_Class_11 Nome_Class_12 Nome_Class_13 Nome_Class_14 Nome_Class_15  \n",
      "0             .         GERAL         GERAL             .        AGENDA  \n",
      "1    CONFECÇÕES     ESPORTIVO         GERAL       INVERNO        ABRIGO  \n",
      "2    CONFECÇÕES         GERAL         GERAL       INVERNO        ABRIGO  \n",
      "3    CONFECÇÕES     ESPORTIVO         GERAL       INVERNO        ABRIGO  \n",
      "4    CONFECÇÕES         GERAL         GERAL       INVERNO        ABRIGO  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregamos as informações referente às classificações dos produtos\n",
    "ds_cl_prod = pd.read_excel(\"DATASET/Produtos/Produtos/Classificacoes_dos_produtos.xlsx\",nrows=900)\n",
    "ds_cl_prod = ds_cl_prod[[\"Classificação\",\"Código\",\"Nome\"]]\n",
    "\n",
    "# Existem 5 tipos de classificações de produtos e para cada uma destas existe um conjunto de valores possíveis, estando tudo misturado no excel carregado acima\n",
    "# Assim, dividimos o data frame carregado em 5 partes, uma para cada classificação, representando o conjunto de valores possível, facilitando a junção dessa informação com as características dos produtos\n",
    "# Além disso, alteramos o nome das colunas, evidenciando aquilo que realmente representam e distinguindo de outras colunas com nomes iguais ou semelhantes possivelmente existentes \n",
    "# após a junção que será efetuada no próximo passo \n",
    "ds_cl_prod_11 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 11].rename(columns={\"Código\":\"Código_Class_11\",\"Nome\":\"Nome_Class_11\"})\n",
    "ds_cl_prod_12 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 12].rename(columns={\"Código\":\"Código_Class_12\",\"Nome\":\"Nome_Class_12\"})\n",
    "ds_cl_prod_13 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 13].rename(columns={\"Código\":\"Código_Class_13\",\"Nome\":\"Nome_Class_13\"})\n",
    "ds_cl_prod_14 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 14].rename(columns={\"Código\":\"Código_Class_14\",\"Nome\":\"Nome_Class_14\"})\n",
    "ds_cl_prod_15 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 15].rename(columns={\"Código\":\"Código_Class_15\",\"Nome\":\"Nome_Class_15\"})\n",
    "\n",
    "# Juntamos a informação mais detalhada de cada classificação consoante o código presente em cada produto para as 5 colunas de classificação que previamente apresentavam apenas o identificador.\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_11[['Código_Class_11','Nome_Class_11']], left_on='Classificação11', right_on='Código_Class_11', how='left').drop(columns=[\"Código_Class_11\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_12[['Código_Class_12','Nome_Class_12']], left_on='Classificação12', right_on='Código_Class_12', how='left').drop(columns=[\"Código_Class_12\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_13[['Código_Class_13','Nome_Class_13']], left_on='Classificação13', right_on='Código_Class_13', how='left').drop(columns=[\"Código_Class_13\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_14[['Código_Class_14','Nome_Class_14']], left_on='Classificação14', right_on='Código_Class_14', how='left').drop(columns=[\"Código_Class_14\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_15[['Código_Class_15','Nome_Class_15']], left_on='Classificação15', right_on='Código_Class_15', how='left').drop(columns=[\"Código_Class_15\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_prod_merged.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb5ea03-de69-43ed-ac82-7b43becec575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Empresa  Emitente Série  Número  Item    Data do Movimento  Operação  \\\n",
      "0        6         0   NaN   17710    47  2018-01-03 00:00:00         1   \n",
      "1        6         0   NaN   17710    48  2018-01-03 00:00:00         1   \n",
      "2        6         0   NaN   17710    49  2018-01-03 00:00:00         1   \n",
      "3        6         0   NaN   17710    50  2018-01-03 00:00:00         1   \n",
      "4        6         0   NaN   17710    51  2018-01-03 00:00:00         1   \n",
      "\n",
      "   Origem ou Destino  Comprador ou Vendedor  Produto  ...  Situação  Pedido  \\\n",
      "0               5005                   15.0   265755  ...         1     NaN   \n",
      "1               5005                   15.0   340832  ...         1     NaN   \n",
      "2               5005                   15.0   237311  ...         1     NaN   \n",
      "3               5005                   15.0   237311  ...         1     NaN   \n",
      "4               5005                   15.0   237311  ...         1     NaN   \n",
      "\n",
      "   Encalhe  Icm do Frete  Preço para Conferência  Capítulo do IPI  \\\n",
      "0      NaN           NaN                     NaN              NaN   \n",
      "1      NaN           NaN                     NaN              NaN   \n",
      "2      NaN           NaN                     NaN              NaN   \n",
      "3      NaN           NaN                     NaN              NaN   \n",
      "4      NaN           NaN                     NaN              NaN   \n",
      "\n",
      "   Série da Devolução  Número da Devolução  Base ST  Valor ST  \n",
      "0                 NaN                  NaN      NaN       NaN  \n",
      "1                 NaN                  NaN      NaN       NaN  \n",
      "2                 NaN                  NaN      NaN       NaN  \n",
      "3                 NaN                  NaN      NaN       NaN  \n",
      "4                 NaN                  NaN      NaN       NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Carregamos as informações referente às movimentações/transações dos produtos (vendas/compras/devoluções/entre outros...), separadas em 9 ficheiros, juntando as mesmas num só data frame.\n",
    "dataset_detMov_2018_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2018_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2018_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2018_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2019_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2019_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2019_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2019_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2020_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2020_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2020_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2020_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2021_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2021_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2021_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2021_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2022 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2022_Esportes.xls\",\"Query\")\n",
    "dataset_detMov_merged = pd.concat([dataset_detMov_2018_1,dataset_detMov_2018_2,dataset_detMov_2019_1,dataset_detMov_2019_2,dataset_detMov_2020_1,dataset_detMov_2020_2,dataset_detMov_2021_1,dataset_detMov_2021_2,dataset_detMov_2022])\n",
    "\n",
    "# Verificamos o conjunto total de movimentações de produtos.\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27e45b1-4b31-4685-96f0-86b88a1f3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Série', 'Número', 'Item', 'Data do Movimento', 'Operação', 'Produto',\n",
      "       'Tamanho', 'Quantidade', 'Valor original'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verificamos as colunas das movimentações, mantendo as colunas necessárias para o projeto, \n",
    "# sendo estas todas as que representam características realmente utilizadas pela empresa e que poderão ser utilizadas tanto para o cálculo de receitas como para a associação de produtos\n",
    "dataset_detMov_merged = dataset_detMov_merged[[\"Série\", \"Número\", \"Item\", \"Data do Movimento\",\"Operação\",\"Produto\",\"Tamanho\",\"Quantidade\",\"Valor original\"]]\n",
    "\n",
    "# Verificamos que as colunas foram devidamente removidas\n",
    "print(dataset_detMov_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330e4298-61af-42b5-8564-9805fd5daef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Série  Número  Item    Data do Movimento  Operação  Produto  Tamanho  \\\n",
      "0   NaN   17710    47  2018-01-03 00:00:00         1   265755      130   \n",
      "1   NaN   17710    48  2018-01-03 00:00:00         1   340832      130   \n",
      "2   NaN   17710    49  2018-01-03 00:00:00         1   237311       95   \n",
      "3   NaN   17710    50  2018-01-03 00:00:00         1   237311      100   \n",
      "4   NaN   17710    51  2018-01-03 00:00:00         1   237311      105   \n",
      "\n",
      "   Quantidade  Valor original Nome_Tamanho  \n",
      "0         3.0        3.314815            U  \n",
      "1         3.0        3.888889            U  \n",
      "2         2.0        7.868519          095  \n",
      "3         2.0        7.868519          100  \n",
      "4         2.0        7.868519          105  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Carregamos as informações referente aos tamanhos dos produtos\n",
    "# Além disso, alteramos o nome das colunas, seguindo a mesma lógica previamente realizada para as classificações\n",
    "ds_tam = pd.read_excel(\"DATASET/Tamanhos/Tamanhos.xls\",\"Tamanhos\")\n",
    "ds_tam = ds_tam.rename(columns={\"Código\":\"Código_Tamanho\",\"Nome\":\"Nome_Tamanho\"})\n",
    "\n",
    "# Juntamos a informação mais detalhada do tamanho consoante o código presente em cada produto para a coluna que previamente apresentava apenas o identificador.\n",
    "dataset_detMov_merged = pd.merge(dataset_detMov_merged,ds_tam, left_on='Tamanho', right_on='Código_Tamanho', how='left').drop(columns=[\"Código_Tamanho\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44258058-76d4-4234-a910-983d6d2a1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Alteramos o nome das colunas, seguindo a mesma lógica previamente realizada para as classificações e tamanhos\n",
    "dataset_prod_merged = dataset_prod_merged.rename(columns={\"Referência\":\"Referência_Produto\", \"Complemento\":\"Complemento_Produto\",\"Código\":\"Código_Produto\",\"Nome\":\"Nome_Produto\", \"Unidade\":\"Unidade_Produto\", \"Data da última atualização\":\"DataUltimaAtualização_Produto\",\"Nome da marca\": \"NomeMarca_Produto\",\"Classificação11\": \"Classificação11_Produto\",\"Classificação12\": \"Classificação12_Produto\",\"Classificação13\": \"Classificação13_Produto\",\"Classificação14\": \"Classificação14_Produto\",\"Classificação15\": \"Classificação15_Produto\", \"Nome_Class_11\": \"Nome_Class_11_Produto\", \"Nome_Class_12\": \"Nome_Class_12_Produto\", \"Nome_Class_13\": \"Nome_Class_13_Produto\", \"Nome_Class_14\": \"Nome_Class_14_Produto\", \"Nome_Class_15\": \"Nome_Class_15_Produto\"})\n",
    "\n",
    "# Juntamos a informação dos produtos com os dados das movimentações consoante o código do produto presente em cada movimentação, que previamente apresentava apenas o identificador.\n",
    "dataset_detMov_merged = pd.merge(dataset_detMov_merged, dataset_prod_merged, left_on='Produto', right_on='Código_Produto', how='left').drop(columns=[\"Código_Produto\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a72bb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Série', 'Número', 'Item', 'Data do Movimento', 'Operação', 'Produto',\n",
      "       'Tamanho', 'Quantidade', 'Preço', 'Nome_Tamanho', 'Nome_Produto',\n",
      "       'Unidade_Produto', 'Referência_Produto',\n",
      "       'DataUltimaAtualização_Produto', 'NomeMarca_Produto',\n",
      "       'Complemento_Produto', 'Nome_Class_11_Produto', 'Nome_Class_12_Produto',\n",
      "       'Nome_Class_13_Produto', 'Nome_Class_14_Produto',\n",
      "       'Nome_Class_15_Produto'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removemos as colunas que não pretendemos utilizar e alteramos o nome da coluna que representa o preço para efeitos de compreensão.\n",
    "dataset_detMov_merged = dataset_detMov_merged.drop(columns=[\"Classificação11_Produto\", \"Classificação12_Produto\", \"Classificação13_Produto\", \"Classificação14_Produto\", \"Classificação15_Produto\"])\n",
    "dataset_detMov_merged = dataset_detMov_merged.rename(columns={\"Valor original\":\"Preço\"})\n",
    "\n",
    "# Verificamos que as alterações foram efetuadas com sucesso\n",
    "print(dataset_detMov_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc77c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03    87440\n",
      "NE    82971\n",
      "04    79683\n",
      "05    31685\n",
      "NS    14746\n",
      "AJ     1832\n",
      "SV       39\n",
      "U        17\n",
      "UE        1\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Existem movimentações/tarnsações de diversos tipos. No entanto, apenas as transações do tipo venda e compra é que são relevantes para o este projeto, sendo estas representadas pelos seguintes valores:\n",
    "# Venda = 03, 04, 05\n",
    "# Compra = NE\n",
    "print(dataset_detMov_merged['Série'].value_counts().sort_values(ascending=False).head(10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf4df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VENDA     198808\n",
      "COMPRA     82971\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assim, mantemos apenas as movimentações/transações que pretendemos utilizar, agrupando os dados de cada tipo em apenas dois tipos, Compra e Venda.\n",
    "dataset_detMov_merged['Série'].replace(['03','04', '05', 'NE'], [\"VENDA\", \"VENDA\", \"VENDA\", \"COMPRA\"], inplace=True)\n",
    "dataset_detMov_merged = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"VENDA\") |  (dataset_detMov_merged['Série'] == \"COMPRA\")]\n",
    "\n",
    "# Verificamos que já só existem movimentações do tipo Compra e Venda.\n",
    "print(dataset_detMov_merged['Série'].value_counts().sort_values(ascending=False).head(10))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b0db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VENDA    198808\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Separamos as vendas das compras\n",
    "dataset_compras = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"COMPRA\")].copy()\n",
    "dataset_vendas = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"VENDA\")].copy()\n",
    "\n",
    "# Verificamos que as vendas estão separadas\n",
    "print(dataset_vendas['Série'].value_counts().sort_values(ascending=False).head(10))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9b55a84-e8e2-4895-bc9d-bf565c092c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Produto_  Total_Receitas_2018Q1  Total_Receitas_2018Q2  \\\n",
      "0      1040               0.000000               0.000000   \n",
      "1      1061             176.666667             294.444444   \n",
      "2      1062               0.000000               0.000000   \n",
      "3      1063               0.000000               0.000000   \n",
      "4      2999             410.425926              99.833333   \n",
      "\n",
      "   Total_Receitas_2018Q3  Total_Receitas_2018Q4  Total_Receitas_2019Q1  \\\n",
      "0               0.000000               0.027778               0.009259   \n",
      "1              88.333333             265.000000             294.444444   \n",
      "2               0.000000              22.037037              44.074074   \n",
      "3               0.000000              47.962963               0.000000   \n",
      "4              88.740741             155.296296             166.388889   \n",
      "\n",
      "   Total_Receitas_2019Q2  Total_Receitas_2019Q3  Total_Receitas_2019Q4  \\\n",
      "0               0.000000               0.018519               0.000000   \n",
      "1             176.666667             257.962963             147.407407   \n",
      "2               0.000000               0.000000             110.185185   \n",
      "3               0.000000               0.000000               0.000000   \n",
      "4              77.648148             232.944444             133.111111   \n",
      "\n",
      "   Total_Receitas_2020Q1  ...  Total_Unidades_Vendidas_2020Q3  \\\n",
      "0               0.000000  ...                             0.0   \n",
      "1             147.407407  ...                             3.0   \n",
      "2             132.222222  ...                             1.0   \n",
      "3               0.000000  ...                             0.0   \n",
      "4             510.259259  ...                            45.0   \n",
      "\n",
      "   Total_Unidades_Vendidas_2020Q4  Total_Unidades_Vendidas_2021Q1  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             1.0                             0.0   \n",
      "2                             2.0                             2.0   \n",
      "3                             0.0                             0.0   \n",
      "4                            45.0                            26.0   \n",
      "\n",
      "   Total_Unidades_Vendidas_2021Q2  Total_Unidades_Vendidas_2021Q3  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             5.0                             0.0   \n",
      "2                             4.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                            38.0                            21.0   \n",
      "\n",
      "   Total_Unidades_Vendidas_2021Q4  Total_Unidades_Vendidas_2022Q1  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             1.0                             2.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                            34.0                            41.0   \n",
      "\n",
      "   Total_Unidades_Vendidas_2022Q2  Total_Unidades_Vendidas_2022Q3  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             1.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                            23.0                            29.0   \n",
      "\n",
      "   Total_Unidades_Vendidas_2022Q4  \n",
      "0                             0.0  \n",
      "1                             0.0  \n",
      "2                             0.0  \n",
      "3                             0.0  \n",
      "4                            15.0  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silve\\AppData\\Local\\Temp\\ipykernel_15268\\186005611.py:12: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  dataset_vendas_by_product_by_trimester = dataset_vendas.groupby([\"Produto\", \"Ano/Trimestre\"])[\"Preco_Total\", \"Quantidade\"].agg([\"sum\"]).reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convertemos a data no tipo datetime para este ser utilizado posteriormente na divisão do total de vendas por períodos temporais.\n",
    "dataset_vendas[\"Data\"] = pd.to_datetime(dataset_vendas[\"Data do Movimento\"])\n",
    "\n",
    "# Adicionamos uma nova coluna com o ano e trimestre de cada venda consoante a respetiva data.\n",
    "dataset_vendas[\"Ano/Trimestre\"] = dataset_vendas[\"Data\"].dt.to_period(\"Q\")\n",
    "dataset_vendas[\"Ano/Trimestre\"] = dataset_vendas[\"Ano/Trimestre\"].astype(str)\n",
    "\n",
    "# Adicionamos uma nova coluna com o total de receitas de cada venda com base na quantidade e preço por unidade (Preço * Quantidade)\n",
    "dataset_vendas = dataset_vendas.assign(Preco_Total=dataset_vendas[\"Preço\"] * dataset_vendas[\"Quantidade\"])\n",
    "\n",
    "# Agrupamos as vendas por produto e dividimos o total de vendas e a média de vendas por dia por trimestre em colunas próprias.\n",
    "dataset_vendas_by_product_by_trimester = dataset_vendas.groupby([\"Produto\", \"Ano/Trimestre\"])[\"Preco_Total\", \"Quantidade\"].agg([\"sum\"]).reset_index()\n",
    "dataset_vendas_by_product_by_trimester.columns = dataset_vendas_by_product_by_trimester.columns.droplevel(1)\n",
    "\n",
    "# Alteramos o nome das colunas obtidas\n",
    "dataset_vendas_by_product_by_trimester = dataset_vendas_by_product_by_trimester.rename(columns={\"Preco_Total\": \"Total_Receitas\", \"Quantidade\": \"Total_Unidades_Vendidas\"})\n",
    "\n",
    "# Preenchemos o data frame obtido acima com linhas a 0 para cada produto para cada trimestre, caso não exista nenhum registo.\n",
    "dataset_vendas_by_product_by_trimester = dataset_vendas_by_product_by_trimester.pivot(index=\"Produto\", columns=\"Ano/Trimestre\", values=[\"Total_Receitas\", \"Total_Unidades_Vendidas\"]).fillna(0)\n",
    "\n",
    "# Resetamos o index para incluir a coluna \"Produto\"\n",
    "dataset_vendas_by_product_by_trimester = dataset_vendas_by_product_by_trimester.reset_index()\n",
    "\n",
    "# Nivelamos o índice multinível das colunas\n",
    "dataset_vendas_by_product_by_trimester.columns = [\"_\".join(col) for col in dataset_vendas_by_product_by_trimester.columns]\n",
    "\n",
    "# Verificamos o resultado final\n",
    "print(dataset_vendas_by_product_by_trimester.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0178dc38-341c-4040-897d-e416516f0327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Produto_  Total_Receitas_2018Q1  Total_Receitas_2018Q2  \\\n",
      "0      1040               0.000000               0.000000   \n",
      "1      1061             176.666667             294.444444   \n",
      "2      1062               0.000000               0.000000   \n",
      "3      1063               0.000000               0.000000   \n",
      "4      2999             410.425926              99.833333   \n",
      "\n",
      "   Total_Receitas_2018Q3  Total_Receitas_2018Q4  Total_Receitas_2019Q1  \\\n",
      "0               0.000000               0.027778               0.009259   \n",
      "1              88.333333             265.000000             294.444444   \n",
      "2               0.000000              22.037037              44.074074   \n",
      "3               0.000000              47.962963               0.000000   \n",
      "4              88.740741             155.296296             166.388889   \n",
      "\n",
      "   Total_Receitas_2019Q2  Total_Receitas_2019Q3  Total_Receitas_2019Q4  \\\n",
      "0               0.000000               0.018519               0.000000   \n",
      "1             176.666667             257.962963             147.407407   \n",
      "2               0.000000               0.000000             110.185185   \n",
      "3               0.000000               0.000000               0.000000   \n",
      "4              77.648148             232.944444             133.111111   \n",
      "\n",
      "   Total_Receitas_2020Q1  ...  Classificação11_Produto  \\\n",
      "0               0.000000  ...                        2   \n",
      "1             147.407407  ...                        2   \n",
      "2             132.222222  ...                        2   \n",
      "3               0.000000  ...                        2   \n",
      "4             510.259259  ...                        2   \n",
      "\n",
      "   Classificação12_Produto  Classificação13_Produto  Classificação14_Produto  \\\n",
      "0                        3                        3                        3   \n",
      "1                        2                        3                        3   \n",
      "2                        2                        3                        3   \n",
      "3                        2                        3                        3   \n",
      "4                        2                        3                        3   \n",
      "\n",
      "   Classificação15_Produto  Nome_Class_11_Produto  Nome_Class_12_Produto  \\\n",
      "0                       12             ACESSÓRIOS                  GERAL   \n",
      "1                       17             ACESSÓRIOS              ESPORTIVO   \n",
      "2                       17             ACESSÓRIOS              ESPORTIVO   \n",
      "3                       17             ACESSÓRIOS              ESPORTIVO   \n",
      "4                      279             ACESSÓRIOS              ESPORTIVO   \n",
      "\n",
      "   Nome_Class_13_Produto  Nome_Class_14_Produto  Nome_Class_15_Produto  \n",
      "0                  GERAL                  ANUAL               ATACADOR  \n",
      "1                  GERAL                  ANUAL                  BARRA  \n",
      "2                  GERAL                  ANUAL                  BARRA  \n",
      "3                  GERAL                  ANUAL                  BARRA  \n",
      "4                  GERAL                  ANUAL             BOLA TÊNIS  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Juntamos a informação dos produtos com os dados das respetivas vendas consoante o código do produto.\n",
    "dataset_treated = pd.merge(dataset_vendas_by_product_by_trimester, dataset_prod_merged, left_on='Produto_', right_on='Código_Produto', how='left').drop(columns=[\"Código_Produto\"]).fillna(0)\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_treated.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ca8a0c4-fb56-424a-908f-59afd69cbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Produto', 'Total_Receitas_2018Q1', 'Total_Receitas_2018Q2',\n",
      "       'Total_Receitas_2018Q3', 'Total_Receitas_2018Q4',\n",
      "       'Total_Receitas_2019Q1', 'Total_Receitas_2019Q2',\n",
      "       'Total_Receitas_2019Q3', 'Total_Receitas_2019Q4',\n",
      "       'Total_Receitas_2020Q1', 'Total_Receitas_2020Q2',\n",
      "       'Total_Receitas_2020Q3', 'Total_Receitas_2020Q4',\n",
      "       'Total_Receitas_2021Q1', 'Total_Receitas_2021Q2',\n",
      "       'Total_Receitas_2021Q3', 'Total_Receitas_2021Q4',\n",
      "       'Total_Receitas_2022Q1', 'Total_Receitas_2022Q2',\n",
      "       'Total_Receitas_2022Q3', 'Total_Receitas_2022Q4',\n",
      "       'Total_Unidades_Vendidas_2018Q1', 'Total_Unidades_Vendidas_2018Q2',\n",
      "       'Total_Unidades_Vendidas_2018Q3', 'Total_Unidades_Vendidas_2018Q4',\n",
      "       'Total_Unidades_Vendidas_2019Q1', 'Total_Unidades_Vendidas_2019Q2',\n",
      "       'Total_Unidades_Vendidas_2019Q3', 'Total_Unidades_Vendidas_2019Q4',\n",
      "       'Total_Unidades_Vendidas_2020Q1', 'Total_Unidades_Vendidas_2020Q2',\n",
      "       'Total_Unidades_Vendidas_2020Q3', 'Total_Unidades_Vendidas_2020Q4',\n",
      "       'Total_Unidades_Vendidas_2021Q1', 'Total_Unidades_Vendidas_2021Q2',\n",
      "       'Total_Unidades_Vendidas_2021Q3', 'Total_Unidades_Vendidas_2021Q4',\n",
      "       'Total_Unidades_Vendidas_2022Q1', 'Total_Unidades_Vendidas_2022Q2',\n",
      "       'Total_Unidades_Vendidas_2022Q3', 'Total_Unidades_Vendidas_2022Q4',\n",
      "       'Nome_Produto', 'Unidade_Produto', 'Referencia_Produto',\n",
      "       'DataUltimaAtualizacao_Produto', 'NomeMarca_Produto',\n",
      "       'Complemento_Produto', 'Classificacao11_Produto',\n",
      "       'Classificacao12_Produto', 'Classificacao13_Produto',\n",
      "       'Classificacao14_Produto', 'Classificacao15_Produto',\n",
      "       'Nome_Class_11_Produto', 'Nome_Class_12_Produto',\n",
      "       'Nome_Class_13_Produto', 'Nome_Class_14_Produto',\n",
      "       'Nome_Class_15_Produto'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import unidecode\n",
    "\n",
    "# Removemos os acentos existentes nos nomes das colunas para efeitos de rigor\n",
    "dataset_treated.columns = [unidecode.unidecode(col) for col in dataset_treated.columns]\n",
    "dataset_treated = dataset_treated.rename(columns={\"Produto_\": \"Produto\"})\n",
    "\n",
    "# Verificamos o resultado final do dataset já totalmente tratado\n",
    "print(dataset_treated.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fbd9748-0756-4dc8-9f0c-6fb0c655966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Exportamos o Dataset já totalmente tratado\n",
    "dataset_treated.to_csv('Dataset_Treated.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dae545736ee78e012acd08c370c6df0e68e8a318016cb2f8480f264d55cb96c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
