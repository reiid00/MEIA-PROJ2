{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f9ad72b2",
   "metadata": {},
   "source": [
    "\n",
    "# Pré-Tratamento: Tratamento de Dados e Preparação do Dataset\n",
    "\n",
    "## Objetivo\n",
    "  A apartir de um conjunto de dados não tratados referente às receitas de uma loja de desporto, efetuar as associações necessárias, eliminando toda a informação considerada desnecessária para o âmbito do nosso projeto, gerando um dataset final a ser, posteriormente, importado no pré-processamento de dados. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fac7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\silve\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install xlrd\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471bbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feaa997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregamos as informações de todos os produtos, separados em 7 ficheiros, juntando os mesmos num só data frame.\n",
    "dataset_prod_1 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_1.xls\",\"Produtos\")\n",
    "dataset_prod_2 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_2.xls\",\"Query\")\n",
    "dataset_prod_3 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_3.xls\",\"Query\")\n",
    "dataset_prod_4 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_4.xls\",\"Query\")\n",
    "dataset_prod_5 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_5.xls\",\"Query\")\n",
    "dataset_prod_6 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_6.xls\",\"Query\")\n",
    "dataset_prod_7 = pd.read_excel(\"DATASET/Produtos/Produtos/Produtos_7.xls\",\"Query\")\n",
    "dataset_prod_merged = pd.concat([dataset_prod_1,dataset_prod_2,dataset_prod_3,dataset_prod_4,dataset_prod_5,dataset_prod_6,dataset_prod_7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59334e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Código                           Nome                      Descrição  \\\n",
      "0       1              TESTE HOMOLOGACAO              TESTE HOMOLOGACAO   \n",
      "1       2      PENALTY 353026-645 TACTEL      PENALTY 353026-645 TACTEL   \n",
      "2       3      PENALTY 353028-650 TACTEL      PENALTY 353028-650 TACTEL   \n",
      "3       4      PENALTY 353028-625 TACTEL      PENALTY 353028-625 TACTEL   \n",
      "4       5  PENALTY 353025-652 MICROFIBRA  PENALTY 353025-652 MICROFIBRA   \n",
      "\n",
      "  Unidade  Fornecedor  Marca  Estoque  Situação Tributária  IPI de Entrada  \\\n",
      "0      UN         NaN    0.0      NaN                  0.0             0.0   \n",
      "1      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "2      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "3      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "4      UN         NaN    0.0      NaN                  NaN             0.0   \n",
      "\n",
      "   IPI de Saída  ...  Classificação13  Classificação14  Classificação15  \\\n",
      "0           0.0  ...                3                0              230   \n",
      "1           0.0  ...                3                2                1   \n",
      "2           0.0  ...                3                2                1   \n",
      "3           0.0  ...                3                2                1   \n",
      "4           0.0  ...                3                2                1   \n",
      "\n",
      "   Marcação       CEST  EAN  NCM  Código do Benefício  \\\n",
      "0         0  1234567.0  NaN  NaN                  NaN   \n",
      "1         0        NaN  NaN  NaN                  NaN   \n",
      "2         0        NaN  NaN  NaN                  NaN   \n",
      "3         0        NaN  NaN  NaN                  NaN   \n",
      "4         0        NaN  NaN  NaN                  NaN   \n",
      "\n",
      "   Descrição para Internet  Habilitado para Internet  \n",
      "0                      NaN                       NaN  \n",
      "1                      NaN                       NaN  \n",
      "2                      NaN                       NaN  \n",
      "3                      NaN                       NaN  \n",
      "4                      NaN                       NaN  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificamos o conjunto total de produtos.\n",
    "print(dataset_prod_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adce8c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Código', 'Nome', 'Unidade', 'Referência', 'Data da última atualização',\n",
      "       'Nome da marca', 'Complemento', 'Classificação11', 'Classificação12',\n",
      "       'Classificação13', 'Classificação14', 'Classificação15'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificamos as colunas dos produtos, mantendo as colunas necessárias para o projeto, \n",
    "# sendo estas todas as que representam características realmente utilizadas pela empresa e que poderão ser utilizadas tanto para o cálculo de receitas como para a associação de produtos\n",
    "dataset_prod_merged = dataset_prod_merged[[\"Código\",\"Nome\",\"Unidade\",\"Referência\",\"Data da última atualização\",\"Nome da marca\", \"Complemento\", \"Classificação11\",\"Classificação12\",\"Classificação13\",\"Classificação14\",\"Classificação15\"]]\n",
    "\n",
    "# Verificamos que as colunas foram devidamente removidas\n",
    "print(dataset_prod_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28133e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Código                           Nome Unidade Referência  \\\n",
      "0       1              TESTE HOMOLOGACAO      UN        NaN   \n",
      "1       2      PENALTY 353026-645 TACTEL      UN        NaN   \n",
      "2       3      PENALTY 353028-650 TACTEL      UN        NaN   \n",
      "3       4      PENALTY 353028-625 TACTEL      UN        NaN   \n",
      "4       5  PENALTY 353025-652 MICROFIBRA      UN        NaN   \n",
      "\n",
      "  Data da última atualização Nome da marca Complemento  Classificação11  \\\n",
      "0        2016-06-27 00:00:00         TESTE         NaN                0   \n",
      "1        2012-06-15 00:00:00       PENALTY         NaN                3   \n",
      "2                        NaT       PENALTY         NaN                3   \n",
      "3                        NaT       PENALTY         NaN                3   \n",
      "4                        NaT       PENALTY         NaN                3   \n",
      "\n",
      "   Classificação12  Classificação13  Classificação14  Classificação15  \\\n",
      "0                3                3                0              230   \n",
      "1                2                3                2                1   \n",
      "2                3                3                2                1   \n",
      "3                2                3                2                1   \n",
      "4                3                3                2                1   \n",
      "\n",
      "  Nome_Class_11 Nome_Class_12 Nome_Class_13 Nome_Class_14 Nome_Class_15  \n",
      "0             .         GERAL         GERAL             .        AGENDA  \n",
      "1    CONFECÇÕES     ESPORTIVO         GERAL       INVERNO        ABRIGO  \n",
      "2    CONFECÇÕES         GERAL         GERAL       INVERNO        ABRIGO  \n",
      "3    CONFECÇÕES     ESPORTIVO         GERAL       INVERNO        ABRIGO  \n",
      "4    CONFECÇÕES         GERAL         GERAL       INVERNO        ABRIGO  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregamos as informações referente às classificações dos produtos\n",
    "ds_cl_prod = pd.read_excel(\"DATASET/Produtos/Produtos/Classificacoes_dos_produtos.xlsx\",nrows=900)\n",
    "ds_cl_prod = ds_cl_prod[[\"Classificação\",\"Código\",\"Nome\"]]\n",
    "\n",
    "# Existem 5 tipos de classificações de produtos e para cada uma destas existe um conjunto de valores possíveis, estando tudo misturado no excel carregado acima\n",
    "# Assim, dividimos o data frame carregado em 5 partes, uma para cada classificação, representando o conjunto de valores possível, facilitando a junção dessa informação com as características dos produtos\n",
    "# Além disso, alteramos o nome das colunas, evidenciando aquilo que realmente representam e distinguindo de outras colunas com nomes iguais ou semelhantes possivelmente existentes \n",
    "# após a junção que será efetuada no próximo passo \n",
    "ds_cl_prod_11 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 11].rename(columns={\"Código\":\"Código_Class_11\",\"Nome\":\"Nome_Class_11\"})\n",
    "ds_cl_prod_12 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 12].rename(columns={\"Código\":\"Código_Class_12\",\"Nome\":\"Nome_Class_12\"})\n",
    "ds_cl_prod_13 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 13].rename(columns={\"Código\":\"Código_Class_13\",\"Nome\":\"Nome_Class_13\"})\n",
    "ds_cl_prod_14 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 14].rename(columns={\"Código\":\"Código_Class_14\",\"Nome\":\"Nome_Class_14\"})\n",
    "ds_cl_prod_15 = ds_cl_prod.loc[ds_cl_prod['Classificação'] == 15].rename(columns={\"Código\":\"Código_Class_15\",\"Nome\":\"Nome_Class_15\"})\n",
    "\n",
    "# Juntamos a informação mais detalhada de cada classificação consoante o código presente em cada produto para as 5 colunas de classificação que previamente apresentavam apenas o identificador.\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_11[['Código_Class_11','Nome_Class_11']], left_on='Classificação11', right_on='Código_Class_11', how='left').drop(columns=[\"Código_Class_11\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_12[['Código_Class_12','Nome_Class_12']], left_on='Classificação12', right_on='Código_Class_12', how='left').drop(columns=[\"Código_Class_12\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_13[['Código_Class_13','Nome_Class_13']], left_on='Classificação13', right_on='Código_Class_13', how='left').drop(columns=[\"Código_Class_13\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_14[['Código_Class_14','Nome_Class_14']], left_on='Classificação14', right_on='Código_Class_14', how='left').drop(columns=[\"Código_Class_14\"])\n",
    "dataset_prod_merged = pd.merge(dataset_prod_merged,ds_cl_prod_15[['Código_Class_15','Nome_Class_15']], left_on='Classificação15', right_on='Código_Class_15', how='left').drop(columns=[\"Código_Class_15\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_prod_merged.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb5ea03-de69-43ed-ac82-7b43becec575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Empresa  Emitente Série  Número  Item    Data do Movimento  Operação  \\\n",
      "0        6         0   NaN   17710    47  2018-01-03 00:00:00         1   \n",
      "1        6         0   NaN   17710    48  2018-01-03 00:00:00         1   \n",
      "2        6         0   NaN   17710    49  2018-01-03 00:00:00         1   \n",
      "3        6         0   NaN   17710    50  2018-01-03 00:00:00         1   \n",
      "4        6         0   NaN   17710    51  2018-01-03 00:00:00         1   \n",
      "\n",
      "   Origem ou Destino  Comprador ou Vendedor  Produto  ...  Situação  Pedido  \\\n",
      "0               5005                   15.0   265755  ...         1     NaN   \n",
      "1               5005                   15.0   340832  ...         1     NaN   \n",
      "2               5005                   15.0   237311  ...         1     NaN   \n",
      "3               5005                   15.0   237311  ...         1     NaN   \n",
      "4               5005                   15.0   237311  ...         1     NaN   \n",
      "\n",
      "   Encalhe  Icm do Frete  Preço para Conferência  Capítulo do IPI  \\\n",
      "0      NaN           NaN                     NaN              NaN   \n",
      "1      NaN           NaN                     NaN              NaN   \n",
      "2      NaN           NaN                     NaN              NaN   \n",
      "3      NaN           NaN                     NaN              NaN   \n",
      "4      NaN           NaN                     NaN              NaN   \n",
      "\n",
      "   Série da Devolução  Número da Devolução  Base ST  Valor ST  \n",
      "0                 NaN                  NaN      NaN       NaN  \n",
      "1                 NaN                  NaN      NaN       NaN  \n",
      "2                 NaN                  NaN      NaN       NaN  \n",
      "3                 NaN                  NaN      NaN       NaN  \n",
      "4                 NaN                  NaN      NaN       NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Carregamos as informações referente às movimentações/transações dos produtos (vendas/compras/devoluções/entre outros...), separadas em 9 ficheiros, juntando as mesmas num só data frame.\n",
    "dataset_detMov_2018_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2018_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2018_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2018_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2019_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2019_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2019_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2019_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2020_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2020_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2020_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2020_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2021_1 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2021_Esportes_1.xls\",\"Query\")\n",
    "dataset_detMov_2021_2 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2021_Esportes_2.xls\",\"Query\")\n",
    "dataset_detMov_2022 = pd.read_excel(\"DATASET/DetalheMovimentacao/DetalheMovimentacao_2022_Esportes.xls\",\"Query\")\n",
    "dataset_detMov_merged = pd.concat([dataset_detMov_2018_1,dataset_detMov_2018_2,dataset_detMov_2019_1,dataset_detMov_2019_2,dataset_detMov_2020_1,dataset_detMov_2020_2,dataset_detMov_2021_1,dataset_detMov_2021_2,dataset_detMov_2022])\n",
    "\n",
    "# Verificamos o conjunto total de movimentações de produtos.\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27e45b1-4b31-4685-96f0-86b88a1f3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Série', 'Número', 'Item', 'Data do Movimento', 'Operação', 'Produto',\n",
      "       'Tamanho', 'Quantidade', 'Valor original'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verificamos as colunas das movimentações, mantendo as colunas necessárias para o projeto, \n",
    "# sendo estas todas as que representam características realmente utilizadas pela empresa e que poderão ser utilizadas tanto para o cálculo de receitas como para a associação de produtos\n",
    "dataset_detMov_merged = dataset_detMov_merged[[\"Série\", \"Número\", \"Item\", \"Data do Movimento\",\"Operação\",\"Produto\",\"Tamanho\",\"Quantidade\",\"Valor original\"]]\n",
    "\n",
    "# Verificamos que as colunas foram devidamente removidas\n",
    "print(dataset_detMov_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330e4298-61af-42b5-8564-9805fd5daef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Série  Número  Item    Data do Movimento  Operação  Produto  Tamanho  \\\n",
      "0   NaN   17710    47  2018-01-03 00:00:00         1   265755      130   \n",
      "1   NaN   17710    48  2018-01-03 00:00:00         1   340832      130   \n",
      "2   NaN   17710    49  2018-01-03 00:00:00         1   237311       95   \n",
      "3   NaN   17710    50  2018-01-03 00:00:00         1   237311      100   \n",
      "4   NaN   17710    51  2018-01-03 00:00:00         1   237311      105   \n",
      "\n",
      "   Quantidade  Valor original Nome_Tamanho  \n",
      "0         3.0        3.314815            U  \n",
      "1         3.0        3.888889            U  \n",
      "2         2.0        7.868519          095  \n",
      "3         2.0        7.868519          100  \n",
      "4         2.0        7.868519          105  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Carregamos as informações referente aos tamanhos dos produtos\n",
    "# Além disso, alteramos o nome das colunas, seguindo a mesma lógica previamente realizada para as classificações\n",
    "ds_tam = pd.read_excel(\"DATASET/Tamanhos/Tamanhos.xls\",\"Tamanhos\")\n",
    "ds_tam = ds_tam.rename(columns={\"Código\":\"Código_Tamanho\",\"Nome\":\"Nome_Tamanho\"})\n",
    "\n",
    "# Juntamos a informação mais detalhada do tamanho consoante o código presente em cada produto para a coluna que previamente apresentava apenas o identificador.\n",
    "dataset_detMov_merged = pd.merge(dataset_detMov_merged,ds_tam, left_on='Tamanho', right_on='Código_Tamanho', how='left').drop(columns=[\"Código_Tamanho\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44258058-76d4-4234-a910-983d6d2a1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Série  Número  Item    Data do Movimento  Operação  Produto  Tamanho  \\\n",
      "0   NaN   17710    47  2018-01-03 00:00:00         1   265755      130   \n",
      "1   NaN   17710    48  2018-01-03 00:00:00         1   340832      130   \n",
      "2   NaN   17710    49  2018-01-03 00:00:00         1   237311       95   \n",
      "3   NaN   17710    50  2018-01-03 00:00:00         1   237311      100   \n",
      "4   NaN   17710    51  2018-01-03 00:00:00         1   237311      105   \n",
      "\n",
      "   Quantidade  Valor original Nome_Tamanho  ... Classificação11_Produto  \\\n",
      "0         3.0        3.314815            U  ...                     2.0   \n",
      "1         3.0        3.888889            U  ...                     2.0   \n",
      "2         2.0        7.868519          095  ...                     3.0   \n",
      "3         2.0        7.868519          100  ...                     3.0   \n",
      "4         2.0        7.868519          105  ...                     3.0   \n",
      "\n",
      "  Classificação12_Produto Classificação13_Produto Classificação14_Produto  \\\n",
      "0                     2.0                     3.0                     3.0   \n",
      "1                     2.0                     3.0                     3.0   \n",
      "2                     2.0                     2.0                     3.0   \n",
      "3                     2.0                     2.0                     3.0   \n",
      "4                     2.0                     2.0                     3.0   \n",
      "\n",
      "  Classificação15_Produto Nome_Class_11_Produto  Nome_Class_12_Produto  \\\n",
      "0                    24.0            ACESSÓRIOS              ESPORTIVO   \n",
      "1                   138.0            ACESSÓRIOS              ESPORTIVO   \n",
      "2                   254.0            CONFECÇÕES              ESPORTIVO   \n",
      "3                   254.0            CONFECÇÕES              ESPORTIVO   \n",
      "4                   254.0            CONFECÇÕES              ESPORTIVO   \n",
      "\n",
      "   Nome_Class_13_Produto  Nome_Class_14_Produto  Nome_Class_15_Produto  \n",
      "0                  GERAL                  ANUAL                   BOIA  \n",
      "1                  GERAL                  ANUAL                POCHETE  \n",
      "2              MASCULINO                  ANUAL            SUNGA SUNGA  \n",
      "3              MASCULINO                  ANUAL            SUNGA SUNGA  \n",
      "4              MASCULINO                  ANUAL            SUNGA SUNGA  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Alteramos o nome das colunas, seguindo a mesma lógica previamente realizada para as classificações e tamanhos\n",
    "dataset_prod_merged = dataset_prod_merged.rename(columns={\"Referência\":\"Referência_Produto\", \"Complemento\":\"Complemento_Produto\",\"Código\":\"Código_Produto\",\"Nome\":\"Nome_Produto\", \"Unidade\":\"Unidade_Produto\", \"Data da última atualização\":\"DataUltimaAtualização_Produto\",\"Nome da marca\": \"NomeMarca_Produto\",\"Classificação11\": \"Classificação11_Produto\",\"Classificação12\": \"Classificação12_Produto\",\"Classificação13\": \"Classificação13_Produto\",\"Classificação14\": \"Classificação14_Produto\",\"Classificação15\": \"Classificação15_Produto\", \"Nome_Class_11\": \"Nome_Class_11_Produto\", \"Nome_Class_12\": \"Nome_Class_12_Produto\", \"Nome_Class_13\": \"Nome_Class_13_Produto\", \"Nome_Class_14\": \"Nome_Class_14_Produto\", \"Nome_Class_15\": \"Nome_Class_15_Produto\"})\n",
    "\n",
    "# Juntamos a informação dos produtos com os dados das movimentações consoante o código do produto presente em cada movimentação, que previamente apresentava apenas o identificador.\n",
    "dataset_detMov_merged = pd.merge(dataset_detMov_merged, dataset_prod_merged, left_on='Produto', right_on='Código_Produto', how='left').drop(columns=[\"Código_Produto\"])\n",
    "\n",
    "# Verificamos que a junção foi realizada com sucesso\n",
    "print(dataset_detMov_merged.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a72bb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Série', 'Número', 'Item', 'Data do Movimento', 'Operação', 'Produto',\n",
      "       'Tamanho', 'Quantidade', 'Preço', 'Nome_Tamanho', 'Nome_Produto',\n",
      "       'Unidade_Produto', 'Referência_Produto',\n",
      "       'DataUltimaAtualização_Produto', 'NomeMarca_Produto',\n",
      "       'Complemento_Produto', 'Nome_Class_11_Produto', 'Nome_Class_12_Produto',\n",
      "       'Nome_Class_13_Produto', 'Nome_Class_14_Produto',\n",
      "       'Nome_Class_15_Produto'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removemos as colunas que não pretendemos utilizar e alteramos o nome da coluna que representa o preço para efeitos de compreensão.\n",
    "dataset_detMov_merged = dataset_detMov_merged.drop(columns=[\"Classificação11_Produto\", \"Classificação12_Produto\", \"Classificação13_Produto\", \"Classificação14_Produto\", \"Classificação15_Produto\"])\n",
    "dataset_detMov_merged = dataset_detMov_merged.rename(columns={\"Valor original\":\"Preço\"})\n",
    "\n",
    "# Verificamos que as alterações foram efetuadas com sucesso\n",
    "print(dataset_detMov_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc77c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03    87440\n",
      "NE    82971\n",
      "04    79683\n",
      "05    31685\n",
      "NS    14746\n",
      "AJ     1832\n",
      "SV       39\n",
      "U        17\n",
      "UE        1\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Existem movimentações/tarnsações de diversos tipos. No entanto, apenas as transações do tipo venda e compra é que são relevantes para o este projeto, sendo estas representadas pelos seguintes valores:\n",
    "# Venda = 03, 04, 05\n",
    "# Compra = NE\n",
    "print(dataset_detMov_merged['Série'].value_counts().sort_values(ascending=False).head(10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf4df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VENDA     198808\n",
      "COMPRA     82971\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assim, mantemos apenas as movimentações/transações que pretendemos utilizar, agrupando os dados de cada tipo em apenas dois tipos, Compra e Venda.\n",
    "dataset_detMov_merged['Série'].replace(['03','04', '05', 'NE'], [\"VENDA\", \"VENDA\", \"VENDA\", \"COMPRA\"], inplace=True)\n",
    "dataset_detMov_merged = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"VENDA\") |  (dataset_detMov_merged['Série'] == \"COMPRA\")]\n",
    "\n",
    "# Verificamos que já só existem movimentações do tipo Compra e Venda.\n",
    "print(dataset_detMov_merged['Série'].value_counts().sort_values(ascending=False).head(10))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b0db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VENDA    198808\n",
      "Name: Série, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Separamos as vendas das compras\n",
    "dataset_compras = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"COMPRA\")].copy()\n",
    "dataset_vendas = dataset_detMov_merged[(dataset_detMov_merged['Série'] == \"VENDA\")].copy()\n",
    "\n",
    "# Verificamos que as vendas estão separadas\n",
    "print(dataset_vendas['Série'].value_counts().sort_values(ascending=False).head(10))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbd9748-0756-4dc8-9f0c-6fb0c655966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Exportamos o Dataset já totalmente tratado\n",
    "dataset_vendas.to_csv('Dataset_Treated.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b99f4c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m rowPrevCustos\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m dataset_vendas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCusto\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_vendas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetCustoFromData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9554\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9556\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9558\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9563\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9564\u001b[0m )\n\u001b[1;32m-> 9565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 873\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    891\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    892\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    893\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [16], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m rowPrevCustos\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m dataset_vendas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCusto\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_vendas\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mgetCustoFromData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [16], line 3\u001b[0m, in \u001b[0;36mgetCustoFromData\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetCustoFromData\u001b[39m(row):\n\u001b[1;32m----> 3\u001b[0m    rowPrevCustos \u001b[38;5;241m=\u001b[39m dataset_compras[\u001b[43m(\u001b[49m\u001b[43mdataset_compras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProduto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProduto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_compras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData do Movimento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData do Movimento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData do Movimento\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreço\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m rowPrevCustos\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:72\u001b[0m, in \u001b[0;36mOpsMixin.__and__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__and__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__and__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logical_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mand_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:6254\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6251\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6252\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6254\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:395\u001b[0m, in \u001b[0;36mlogical_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# For int vs int `^`, `|`, `&` are bitwise operators and return\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m#   integer dtypes.  Otherwise these are boolean ops\u001b[39;00m\n\u001b[0;32m    393\u001b[0m filler \u001b[38;5;241m=\u001b[39m fill_int \u001b[38;5;28;01mif\u001b[39;00m is_self_int_dtype \u001b[38;5;129;01mand\u001b[39;00m is_other_int_dtype \u001b[38;5;28;01melse\u001b[39;00m fill_bool\n\u001b[1;32m--> 395\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mna_logical_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[0;32m    397\u001b[0m res_values \u001b[38;5;241m=\u001b[39m filler(res_values)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:305\u001b[0m, in \u001b[0;36mna_logical_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mna_logical_op\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray, y, op):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# For exposition, write:\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;66;03m#  yarr = isinstance(y, np.ndarray)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;66;03m# Then Cases where this goes through without raising include:\u001b[39;00m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;66;03m#  (xint or xbool) and (yint or bool)\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;66;03m# bool-bool dtype operations should be OK, should not get here\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Inferimos o custo do produto vendido em cada venda com base na compra ao fornecedor mais recente perante a respetiva data\n",
    "def getCustoFromData(row):\n",
    "   rowPrevCustos = dataset_compras[(dataset_compras['Produto'] == row['Produto']) & (dataset_compras['Data do Movimento'] <= row['Data do Movimento'])].sort_values(by='Data do Movimento', ascending=False)['Preço']\n",
    "   if rowPrevCustos.empty:\n",
    "       return -1\n",
    "   return rowPrevCustos.iloc[0]\n",
    "    \n",
    "dataset_vendas['Custo'] = dataset_vendas.apply(lambda row: getCustoFromData(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "633a68a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Custo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Custo'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Verificamos a quantidade de vendas cujo custo não foi inferido\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset_vendas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCusto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))  \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Custo'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificamos a quantidade de vendas cujo custo não foi inferido\n",
    "print(dataset_vendas['Custo'].value_counts().sort_values(ascending=False).head(10))  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a76208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Série  Número  Item    Data do Movimento  Operação  Produto  Tamanho  \\\n",
      "4829  VENDA  771944     1  2018-01-02 00:00:00       104   340822      400   \n",
      "4830  VENDA  771944     2  2018-01-02 00:00:00       104   352722      400   \n",
      "4831  VENDA  771945     1  2018-01-02 00:00:00       104   130090      130   \n",
      "4832  VENDA  771945     2  2018-01-02 00:00:00       104   130090      130   \n",
      "4833  VENDA  771949     1  2018-01-02 00:00:00       102   351559      410   \n",
      "4834  VENDA  771954     1  2018-01-02 00:00:00       105   344441      134   \n",
      "4835  VENDA  771970     1  2018-01-02 00:00:00       104   327673      134   \n",
      "4836  VENDA  771970     2  2018-01-02 00:00:00       104   329892      133   \n",
      "4837  VENDA  771971     1  2018-01-02 00:00:00       104   351537      350   \n",
      "4838  VENDA  771972     1  2018-01-02 00:00:00       101   107743      130   \n",
      "\n",
      "      Quantidade      Preço Nome_Tamanho  ... Unidade_Produto  \\\n",
      "4829         1.0  23.888889           40  ...              PR   \n",
      "4830         1.0  44.259259           40  ...              PR   \n",
      "4831         1.0  18.518519            U  ...              UN   \n",
      "4832         1.0  18.518519            U  ...              UN   \n",
      "4833         1.0  46.111111           41  ...              PR   \n",
      "4834         1.0  33.148148           GG  ...              UN   \n",
      "4835         1.0  12.018519           GG  ...              UN   \n",
      "4836         1.0  12.944444            G  ...              UN   \n",
      "4837         1.0  35.000000           35  ...              PR   \n",
      "4838         1.0  11.092593            U  ...              UN   \n",
      "\n",
      "     Referência_Produto DataUltimaAtualização_Produto NomeMarca_Produto  \\\n",
      "4829         CT00010001           2022-05-14 00:00:00          ALL STAR   \n",
      "4830         CT04480002           2018-08-23 00:00:00          ALL STAR   \n",
      "4831            100,00            2012-02-01 00:00:00                CP   \n",
      "4832            100,00            2012-02-01 00:00:00                CP   \n",
      "4833             FLYTER           2018-09-13 00:00:00         OLYMPIKUS   \n",
      "4834         848865/100           2017-08-11 00:00:00              NIKE   \n",
      "4835             AJ5882           2020-03-18 00:00:00            ADIDAS   \n",
      "4836         719759/435           2017-12-28 00:00:00              NIKE   \n",
      "4837          DELICATE3           2018-11-12 00:00:00         OLYMPIKUS   \n",
      "4838              18010           2021-12-18 00:00:00            SPEEDO   \n",
      "\n",
      "     Complemento_Produto Nome_Class_11_Produto Nome_Class_12_Produto  \\\n",
      "4829                 BCO              CALÇADOS             ESPORTIVO   \n",
      "4830                 PTO              CALÇADOS             ESPORTIVO   \n",
      "4831                  ..            ACESSÓRIOS                 GERAL   \n",
      "4832                  ..            ACESSÓRIOS                 GERAL   \n",
      "4833             PTO/CZA              CALÇADOS             ESPORTIVO   \n",
      "4834           INTER BCO            CONFECÇÕES             ESPORTIVO   \n",
      "4835                 ROY            CONFECÇÕES             ESPORTIVO   \n",
      "4836                 ROY            CONFECÇÕES             ESPORTIVO   \n",
      "4837           CHU/CORAL              CALÇADOS             ESPORTIVO   \n",
      "4838                AZUL            ACESSÓRIOS             ESPORTIVO   \n",
      "\n",
      "     Nome_Class_13_Produto Nome_Class_14_Produto Nome_Class_15_Produto  \n",
      "4829                 GERAL                 ANUAL          TENIS CASUAL  \n",
      "4830                 GERAL                 ANUAL          TENIS CASUAL  \n",
      "4831                 GERAL                 ANUAL                 DIV *  \n",
      "4832                 GERAL                 ANUAL                 DIV *  \n",
      "4833             MASCULINO                 ANUAL          TENIS RUNING  \n",
      "4834             MASCULINO                 ANUAL         CALCAO CALCAO  \n",
      "4835             MASCULINO                 ANUAL         CALCAO CALCAO  \n",
      "4836              FEMININO                 ANUAL         CALCAO CALCAO  \n",
      "4837              FEMININO                 ANUAL          TENIS RUNING  \n",
      "4838                 GERAL                 ANUAL                OCULOS  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_vendas.head(10))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dae545736ee78e012acd08c370c6df0e68e8a318016cb2f8480f264d55cb96c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
